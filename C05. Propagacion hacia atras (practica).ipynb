{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagación hacia atrás\n",
    "\n",
    "En esta sección vamos a crear nuestra primera red neuronal capaz de aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer un ejemplo relativamente sencillo, vamos a resolver un problema de clasificación binaria usando el dataset del Cancer de Mama, que ya hemos visto anteriormente. Recordemos que es un dataset donde las variables independientes son medidas hechas de una imágen de un posible tumor y la variable objetivo es un 0 (cancer maligno) o un 1 (cancer benigno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos solo las 4 primeras variables para que el input coincida con la capa de entrada de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar una red neuronal es muy importante que los datos estén normalizados, si no lo son la red le va a dar más importancia a aquellas variables cuyo rango sea más grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_estandardizador = StandardScaler()\n",
    "X_std = x_estandardizador.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = X_std[0]\n",
    "y0 = y[0]\n",
    "print(x0, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear la siguente red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E implementaremos el algoritmo de propagación hacia atrás (backpropagation) que es lo que permitirá que la red aprenda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, una capa de entrada con 4 neuronas (*también llamadas **unidades**^), una capa oculta con 5 neuronas y una capa de salida que convertirá los outputs (o **activaciones** de la capa oculta en clase positiva o negativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar definimos las funciones de activación distintas:\n",
    "- ** Función identidad**, que se usa en la capa de entrada y no hace nada (o sea, $f(x)=x$)\n",
    "- **Función Sigmoide**, que aplica la función sigmoide $f(x)=\\frac{1}{1+e^{-x}}$ que convierte los números al rango `[0,1]` y que se usa para problemas de clasificación binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_identidad(x, derivada=False):\n",
    "    if derivada:\n",
    "        return np.ones(x.shape)\n",
    "    return x\n",
    "                \n",
    "def fn_sigmoide(x, derivada=False):\n",
    "    if derivada:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "también tenemos que definir una manera de computar el error de una predicción (error, coste y perdida se usan indistintamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un problema de clasificación binaria, una buena métrica es la pérdida logarítmica ([logloss](http://wiki.fast.ai/index.php/Log_Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_logloss(y_pred, y):\n",
    "    p = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    if y == 1:\n",
    "        return -np.log(p)\n",
    "    else:\n",
    "        return -np.log(1 - p)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, definimos la capa básica, que tiene un número de unidades un bias, y una función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_unidades, fn_activacion, bias=True):\n",
    "        self.n_unidades = n_unidades\n",
    "        self.fn_activacion = fn_activacion\n",
    "        self.dim_output = n_unidades\n",
    "        \n",
    "        # añadimos un peso más para la unidad de bias\n",
    "        self.bias = bias\n",
    "\n",
    "        self.dimensiones = \"no generada\"\n",
    "        self.w = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"\"\"\n",
    "        Capa {}. dimensiones = {}.\n",
    "        pesos: {}\n",
    "        \"\"\".format(\n",
    "        self.nombre, self.dimensiones, self.w)\n",
    "    \n",
    "    def generar_pesos(self, dim_output_anterior):\n",
    "        if self.bias:\n",
    "            self.dimensiones = (self.n_unidades, dim_output_anterior+1)\n",
    "        else:\n",
    "            self.dimensiones = (self.n_unidades, dim_output_anterior)\n",
    "        self.w = np.random.random(self.dimensiones)\n",
    "\n",
    "    def add_bias(self, x):\n",
    "        if not self.bias:\n",
    "            return x\n",
    "        x_con_bias_1d = np.append(1, x)\n",
    "        # append convierte en array 1dimensional necesitamos 2d\n",
    "        return x_con_bias_1d.reshape(\n",
    "             x_con_bias_1d.shape[0], 1\n",
    "        )\n",
    "    \n",
    "    def activar(self, x):\n",
    "        x_con_bias_2d = self.add_bias(x)\n",
    "        return self.fn_activacion( self.w @ x_con_bias_2d )\n",
    "\n",
    "    def calcular_delta(self, producto_capa, output_capa):\n",
    "        return producto_capa * self.fn_activacion(output_capa, derivada=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 3 tipos de Capas de neuronas.\n",
    "\n",
    "- **Capa de Entrada**, no hace nada, simplemente conecta el input con el resto de la red\n",
    "- **Capa Oculta**, también llamada capa densa, realiza el algoritmo perceptrón con una función de activación no lineal\n",
    "- **Capa de Salida**, esta capa traduce el output de la capa antepenúltima a la variable objetivo deseada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    nombre = \"entrada\"\n",
    "\n",
    "    def generar_pesos(self):\n",
    "        pass      \n",
    "    \n",
    "    def activar(self, x):\n",
    "        return x\n",
    "\n",
    "class HiddenLayer(Layer):\n",
    "    nombre = \"oculta\"\n",
    "    \n",
    "class OutputLayer(Layer):\n",
    "    nombre = \"salida\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos la red neuronal, que es simplemente una lista de capas y con capacidad de hacer propagación hacia delante y hacia atrás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    def __init__(self, ratio_aprendizaje, fn_error):\n",
    "        self.layers = []\n",
    "        self.ratio_aprendizaje = ratio_aprendizaje\n",
    "        self.fn_error = fn_error\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        if layer.nombre == \"entrada\":\n",
    "            layer.generar_pesos()\n",
    "        else:\n",
    "            layer.generar_pesos(self.layers[-1].dim_output)\n",
    "        self.layers.append(layer)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        info_red = \"\"\n",
    "        for layer in self.layers:\n",
    "            info_red += \"\\nCapa: {} Nº unidades: {}\".format(\n",
    "                        layer.nombre, layer.n_unidades)\n",
    "        return info_red\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.input = layer.add_bias(x).T\n",
    "            x = layer.activar(x)\n",
    "            layer.output = x\n",
    "        return x\n",
    "    \n",
    "    def calcular_error_prediccion(self, y_pred, y):\n",
    "        return self.fn_error(y_pred, y)\n",
    "    \n",
    "    def backward(self, y_pred, y):\n",
    "        # El error de prediccion final\n",
    "        delta_capa = self.calcular_error_prediccion(y_pred, y)\n",
    "        for layer in reversed(self.layers):\n",
    "            if layer.nombre == \"entrada\":\n",
    "                continue    \n",
    "            if layer.nombre == \"salida\":\n",
    "                producto_capa = delta_capa @ layer.w\n",
    "            else:\n",
    "                #quitamos el error del bias de la capa anterior\n",
    "                producto_capa = delta_capa[:,1:] @ layer.w\n",
    "            delta_capa = layer.calcular_delta(producto_capa, layer.output) \n",
    "            layer.delta = delta_capa       \n",
    "                   \n",
    "                \n",
    "    def actualizar_pesos(self):\n",
    "        \"\"\"\n",
    "        Actualiza pesos mediante el descenso de gradiente\"\"\"\n",
    "        for layer in self.layers[1:]:\n",
    "            layer.w = layer.w - self.ratio_aprendizaje \\\n",
    "                      *layer.delta * layer.input\n",
    "\n",
    "    def aprendizaje(self, x, y):\n",
    "        \"\"\"\n",
    "        Función principal para entrenar la red\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(x)\n",
    "        self.backward(y_pred, y)\n",
    "        self.actualizar_pesos()\n",
    "        error_prediccion = self.calcular_error_prediccion(y_pred, y)\n",
    "        return error_prediccion\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probabilidad = self.predict_proba(x)\n",
    "        if probabilidad>=0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar tenemos que definir los tamaños de cada capa, y si van a incluir sesgo (bias) o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 4\n",
    "n_oculta = 5\n",
    "n_output = 1\n",
    "\n",
    "RATIO_APRENDIZAJE = 0.0001\n",
    "N_ITERACIONES=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sigmoide = RedNeuronal(ratio_aprendizaje=RATIO_APRENDIZAJE, fn_error=error_logloss)\n",
    "\n",
    "red_sigmoide.add_layer(InputLayer(n_input, bias=False, fn_activacion=fn_identidad))\n",
    "red_sigmoide.add_layer(HiddenLayer(n_oculta, fn_activacion=fn_sigmoide))\n",
    "red_sigmoide.add_layer(OutputLayer(n_output, fn_activacion=fn_sigmoide))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente la red tiene unos pesos aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sigmoide.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora hacemos una iteración del proceso de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sigmoide.aprendizaje(x0, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los pesos de las capas se han actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sigmoide.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es el equivalente a hacer los siguientes pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = red_sigmoide.forward(x0)\n",
    "prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sigmoide.backward(prediccion, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sigmoide.actualizar_pesos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sigmoide.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos una red neuronal que aprende para optimizar una observación usando el método del descenso de gradiente. Ahora solo tenemos que implementar el método de descenso estocástico de gradiente (SGD) para iterar en todo el dataset de entrenamiento e ir modificando los pesos para minimizar los errores de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteracion_sgd(red, X, y):\n",
    "    # barajamos los datos de entrenamiento\n",
    "    indice_aleatorio = np.random.permutation(X.shape[0])\n",
    "    error =  []\n",
    "    # iteramos todo el dataset\n",
    "    for i in range(indice_aleatorio.shape[0]):\n",
    "        x0 = X[indice_aleatorio[i]]\n",
    "        y0 = y[indice_aleatorio[i]]\n",
    "        err = red.aprendizaje(x0, y0)\n",
    "        error.append(err)\n",
    "    return np.nanmean(np.array(error))\n",
    "\n",
    "def entrenar_sgd(red, n_epocas, X, y):\n",
    "    epocas = []\n",
    "    for epoca in range(n_epocas):\n",
    "        error_epoca = iteracion_sgd(red, X, y)\n",
    "        epocas.append([epoca, error_epoca])\n",
    "    return np.array(epocas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora por ejemplo corremos el algoritmo durante varias iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_sigmoide = entrenar_sgd(red_sigmoide, N_ITERACIONES, X_std, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora visualizamos la evolución del error medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=resultados_sigmoide[:,0], y=resultados_sigmoide[:,1])\n",
    "plt.title(\"Error para red con funcion sigmoide en capa oculta\")\n",
    "plt.xlabel(\"Número de Iteraciones\")\n",
    "plt.ylabel(\"Error medio\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que a cada iteración (época) de aprendizaje el error medio total se va reduciendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo bueno de las redes neuronales es que tienen una flexibilidad que otros modelos no tienen.\n",
    "\n",
    "Por ejemplo podemos cambiar la función de activación de la capa oculta.\n",
    "\n",
    "En la práctica la función sigmoide no se usa para capas ocultas, se suele usar más la **Unidad Linear rectificada (ReLU)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_relu(x, derivada=False):\n",
    "    if derivada:\n",
    "        return 1. * (x>0.)\n",
    "    return np.maximum(x, 0.)\n",
    "\n",
    "def fn_leakyrelu(x, derivada=False):\n",
    "    if derivada:\n",
    "        if x.any()>0:\n",
    "            return 1.\n",
    "        else:\n",
    "            return 0.01\n",
    "    return np.maximum(x, 0.01*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o incluso modificar la red y añadir otra capa con el doble de unidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_relu = RedNeuronal(ratio_aprendizaje=RATIO_APRENDIZAJE, fn_error=error_logloss)\n",
    "red_relu.add_layer(InputLayer(n_input, bias=False, fn_activacion=fn_identidad))\n",
    "red_relu.add_layer(HiddenLayer(n_oculta, fn_activacion=fn_relu))\n",
    "red_relu.add_layer(HiddenLayer(n_oculta, fn_activacion=fn_relu))\n",
    "red_relu.add_layer(OutputLayer(n_output, fn_activacion=fn_sigmoide))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_relu = entrenar_sgd(red_relu, N_ITERACIONES, X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=resultados_relu[:,0], y=resultados_relu[:,1])\n",
    "plt.title(\"Error para red con funcion ReLU en capa oculta\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
