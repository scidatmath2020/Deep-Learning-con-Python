{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Keras\n",
    "Vamos a ver cómo usar Keras para crear modelos de Deep Learning así como sus funcionalidades básicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X = data.data[:,:4]\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "y = y.reshape(569,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras por si mismo no se encarga de hacer todoas las operaciones de bajo nivel (operaciones matriciales), sino que soporta varios backends (el motor que hará el entrenamiento), podemos elegir el que queremos activando la variable de entorno `KERAS_BACKEND`.\n",
    "\n",
    "Keras soporta los siguientes backends:\n",
    "\n",
    "- [theano](http://deeplearning.net/software/theano/): Librería de deep learning original de python para deep learning. Hoy en dia raramente se usa por si sola.\n",
    "- [tensorflow](http://www.tensorflow.org/): Librería de deep learning desarrollada por google. \n",
    "- [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/) Librería de deep learning desarrollada por Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"theano\" #tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](im27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ésta red neuronal se implementa facilmente con Keras, usando la clase `Sequential`, que es similar a la clase `RedNeuronal` que implementamos a mano. Simplemente admite un conjunto de capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "modelo = Sequential()\n",
    "\n",
    "modelo.add(Dense(units=5, activation='sigmoid', input_shape=(4,)))\n",
    "#modelo.add(Dense(units=3, activation='sigmoid'))\n",
    "modelo.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente podemos crear el modelo con las capas directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Sequential([\n",
    "    Dense(units=5, activation='sigmoid', input_dim=4),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora solo queda compilar el modelo y ya quedará preparado para entrenar. A la hora de compilar tenemos que definir la función de pérdidas que medirá el error propagado. \n",
    "\n",
    "\n",
    "Keras tambien nos permite especificar métricas que calculará para cada batch de entrenamiento y nos las dará como un historial despues de entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos añadir el optimizador como string si queremos usarlo con sus hiperparámetros por defecto (es decir, no queremos modificar su ratio de aprendizaje o cualquier otro hiperparámetro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos modificar los parámetros del optimizador tenemos que crear el objeto optimizador. Keras soporta SGD pero tambien muchos otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "sgd = SGD(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=sgd,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver una descripción del modelo con `summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tiene 31 parametros para entrenar, esto se corresponde con los pesos de la red.\n",
    "\n",
    "(4x5 + 5bias  + 5x1 + 1 = 31 pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos ajustar el modelo a los datos de entrenamiento con el método `fit`.  Es importante notar que por defecto keras hace **mini batch**, es decir, no entrena con observaciones individuales, sino con grupos de observaciones (definido el tamaño de los grupos con el parámetro `batch_size`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historial = modelo.fit(X_std, y , epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la evolución del funcionamiento del modelo desde el historial de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historial.history[\"accuracy\"])\n",
    "plt.title(\"Exactitud vs épocas de entrenamiento\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos que no imprima los logs, podemos pasarle al método `fit` el argumento `verbose=0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar el método `predict` como si fuese un estimador de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.predict(X_std)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O si queremos predecir las clases directamente podemos usar `predict_classes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.predict_classes(X_std)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar el funcionamiento del modelo usando `evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = modelo.evaluate(X_std, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "`keras` soporta callbacks, que son simplemente funciones que podemos hacer que se ejecuten en cada paso del proceso de entrenamiento. \n",
    "\n",
    "Básicamente son clases que heredan de `keras.callbacks.Callback`, con los siguientes métodos disponibles:\n",
    "\n",
    "- `on_train_begin()` : se ejecuta al iniciar el entrenamiento\n",
    "- `on_batch_begin()`: se ejecuta al empezar el entrenamiento de un batch (mini batch)\n",
    "- `on_batch_end()`: se ejecuta al acabar un batch (mini batch)\n",
    "- `on_epoch_begin()`: se ejecuta al empezar una época de entrenamiento\n",
    "- `on_epoch_end()`: se ejecuta al acabar una época de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, Keras calcula lás métricas en cada batch, supongamos que queremos calcular una métrica por época (que es más representativo que hacerlo en un batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "class MetricasEpoca(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1_epoca = []\n",
    "        self.recall_epoca = []\n",
    "        self.precision_epoca = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = self.model.predict_classes(self.validation_data[0])\n",
    "        val_targ = self.validation_data[1]\n",
    "        f1 = f1_score(val_targ, val_predict)\n",
    "        recall = recall_score(val_targ, val_predict)\n",
    "        precision = precision_score(val_targ, val_predict)\n",
    "        self.f1_epoca.append(f1)\n",
    "        self.recall_epoca.append(recall)\n",
    "        self.precision_epoca.append(precision)\n",
    "        \n",
    "        \n",
    "modelo = Sequential([\n",
    "    Dense(units=5, activation='sigmoid', input_dim=4),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "modelo.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "metricas_epoca = MetricasEpoca()\n",
    "\n",
    "modelo.fit(X_std, y, validation_data=(X_std, y),\n",
    "           epochs=50, verbose=0, callbacks=[metricas_epoca]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metricas_epoca.f1_epoca)\n",
    "plt.title(\"Metrica F1 vs numero de epocas\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "El entrenamiento de un modelo de deep learning es iterativo, esto significa que en teoría podemos dejar el modelo aprendiendo indefinidamente. En el caso de usar descenso estocástico de gradiente (SGD) para aprender, el error simplemente continuará dando vueltas alrededor del mínimo error.\n",
    "\n",
    "Para evitar tener que entrenar durante el número definido de épocas si el modelo ya ha convergido antes al mínimo de error, podemos implementar lo que se llama `early stopping`. Básicamente, esto para el entrenamiento cuando se cumplen ciertas condiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros principales del EarlyStopping son los siguientes:\n",
    "- **monitor**: La métrica a monitorizar\n",
    "- **min_delta**: la mínima cantidad de variación entre épocas de la métrica para considerarlo un progreso (y continuar entrenando)\n",
    "- **patience**: número de épocas sin mejora despues de las cuales se para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='accuracy', min_delta=0.00001, patience=10,\n",
    "                          verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "modelo = Sequential([\n",
    "    Dense(units=5, activation='sigmoid', input_dim=4),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "modelo.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "modelo.fit(X_std, y, epochs=100, \n",
    "           verbose=1, callbacks=[earlystop]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardado de modelos en Keras.\n",
    "\n",
    "En Keras, podemos guardar un modelo (de forma similar a como haciamos con `joblib/pickle` en `scikit-learn` tanto durante el proceso de entrenamiento (checkpoints) como al acabar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='modelo.hdf5', verbose=1, period=10)\n",
    "\n",
    "\n",
    "modelo = Sequential([\n",
    "    Dense(units=5, activation='sigmoid', input_dim=4),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "modelo.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[\"acc\"])\n",
    "\n",
    "nvo_modelo = modelo.fit(X_std, y, epochs=100, \n",
    "           verbose=1, callbacks=[checkpoint]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos recargar el modelo guardado con `load_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_recargado = load_model(\"modelo.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_recargado.predict(X_std)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación Cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear Redes en keras de forma que sean compatibles con [Scikit-learn](https://keras.io/scikit-learn-api/)\n",
    "\n",
    "En Deep Learning, en  general no se suele hacer validación cruzada a menos que el dataset sea pequeño, ya que los tiempos de entrenamiento de modelos y los datasets suelen ser bastante elevados. No obstante si podemos permitirnoslo es aconsejable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def generar_modelo():\n",
    "    modelo = Sequential()\n",
    "    modelo.add(Dense(units=5, activation='sigmoid', input_dim=4))\n",
    "    modelo.add(Dense(units=1, activation='sigmoid'))\n",
    "    learning_rate = 0.01\n",
    "    sgd = SGD(lr=learning_rate)\n",
    "    modelo.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "    return modelo\n",
    "\n",
    "kfold = StratifiedKFold()\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X_std, y):\n",
    "    modelo = generar_modelo()\n",
    "    modelo.fit(X_std[train], y[train], epochs=100, verbose=0)\n",
    "    scores = modelo.evaluate(X_std[test], y[test], verbose=0)\n",
    "    cvscores.append(scores[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StratifiedKFold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cvscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def generar_modelo(n_oculta=5,activacion=\"sigmoid\"):\n",
    "    modelo = Sequential()\n",
    "    modelo.add(Dense(units = n_oculta, activation = activacion, input_dim = 4))\n",
    "    modelo.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "    sgd = SGD(lr=0.0001)\n",
    "    modelo.compile(loss=\"binary_crossentropy\",optimizer = sgd,metrics=[\"accuracy\"])\n",
    "    return modelo\n",
    "\n",
    "modelo = KerasClassifier(build_fn=generar_modelo,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También vamos a añadir una variable de entorno para controlar cómo funciona la búsqueda de malla. Generalmente pasamos el argumento `n_jobs=-1` que le indica a sklearn que puede usar todos los núcleos de nuestro ordenador. Esto puede dar problemas al ejecutar el código desde Jupyter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOBLIB_START_METHOD\"] = \"forkserver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"epochs\" : [10,30],\n",
    "    \"n_oculta\" : [5,20],\n",
    "    \"activacion\" : [\"sigmoid\",\"relu\"]\n",
    "    \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator = modelo, param_grid = param_grid, scoring = \"accuracy\")\n",
    "grid_result = grid.fit(X_std,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mejor estimador (error {:.5f}): {}\".format(grid_result.best_score_,\n",
    "                                                  grid_result.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
