{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso de gradiente\n",
    "\n",
    "Se trata de un método de optimización local. Es decir, un método para encontrar un máximo o un mínimo locales de una función.\n",
    "\n",
    "Generalmente se le utiliza para minimizar una función de pérdidas, que en nuestro caso es el error del modelo. Por lo tanto, es una de las piezas fundamentales de cómo las redes neuronales aprenden a encontrar pesos óptimos.\n",
    "\n",
    "Supongamos que tenemos la siguiente función: $f$ dada por $f(x)=x^2-2x+4$, y queremos encontrar su mínimo.\n",
    "![imagenes](im10.png)\n",
    "\n",
    "De nuestras clases de cálculo, sabemos que una forma es hacer $f^\\prime(x)=0$, de donde tenemos $2x-2=0$ y por lo tanto en $x=1$ se encuentra el mínimo de la función. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema se puede plantear de manera mucho más compleja: supongamos que $F:\\mathbb{R}^n\\to\\mathbb{R}^m$ viene dada por $$F(x_1,x_2,...,x_n)=(f_1(x_1,...,x_n),f_2(x_1,...,x_n),...,f_m(x_1,...,x_n)).$$ Es decir, $F$ es una función de variable vectorial y valor vectorial.\n",
    "\n",
    "Entonces, encontrar el mínimo de $F$ equivale a resolver el sistema\n",
    "$$\\left(\\begin{matrix}\\frac{\\partial f_1}{\\partial x_1}&\\frac{\\partial f_1}{\\partial x_2}&...&\\frac{\\partial f_1}{\\partial x_n}\\\\\\frac{\\partial f_2}{\\partial x_1}&\\frac{\\partial f_2}{\\partial x_2}&...&\\frac{\\partial f_2}{\\partial x_n}\\\\\\vdots&\\vdots&\\vdots&\\vdots\\\\\\frac{\\partial f_m}{\\partial x_1}&\\frac{\\partial f_m}{\\partial x_2}&...&\\frac{\\partial f_m}{\\partial x_n}\\end{matrix}\\right)=\\left(\\begin{matrix}0\\\\0\\\\\\vdots\\\\0\\end{matrix}\\right)$$\n",
    "\n",
    "Debido a múltiples razones, computacionalmente no siempre es posible resolver el sistema anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa a este problema, se tiene el **Método del gradiente**. Este consiste en resolver de manera iterativa, para funciones de una variable real a los reales, de la siguiente manera: $$x_{i+1}=x_i-\\alpha f^\\prime(x_i)$$ donde $\\alpha$ la *magnitud de paso* que indica cuánto ha de modificarse $x_i$ en cada iteración.\n",
    "\n",
    "Por ejemplo, supongamos que se elige $x_1=3$ (este se elige al azar) y $\\alpha=0.02$. Como $f^\\prime(x)=2x-2$ entonces $x_2=3-0.02(2\\cdot3-2)=2.92$. De la misma manera se obtiene $x_3=2.84$, $x_4=2.76$, etc.\n",
    "\n",
    "Si para cada $i$ graficamos el punto $(i,x_i)$, obtenemos la siguiente gráfica:\n",
    "![imagenes](im11.png)\n",
    "\n",
    "Observamos que $\\lim_{i\\to\\infty}x_i=1$, que es donde sabíamos que se alcanzaba el mínimo de la función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso mas complicado de $F(x_1,x_2,...,x_n)=(f_1(x_1,...,x_n),f_2(x_1,...,x_n),...,f_m(x_1,...,x_n))$, l-a iteración se escribe como $$\\boldsymbol{x_{i+1}}=\\boldsymbol{x_i}-\\alpha\\nabla(J(\\boldsymbol{x_i}))$$\n",
    "\n",
    "Esto es lo que se conoce como **descenso de gradiente en grupo** (batch descent gradient). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
